# Are truthful models more dangerous?
Does activation steering on a Mistral LLM with e.g., the vector that resembles true statements - false statements, lead to increased compliance with harmful requests?

For a cleaner write up, see [here](https://docs.google.com/document/d/1DBBbs4liEFxZPRzP2Ks-cb4M1Yf8NxqAohEz-kb1E1A/edit?usp=sharing).
